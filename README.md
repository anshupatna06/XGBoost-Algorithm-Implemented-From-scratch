# XGBoost-Algorithm-Implemented-From-scratch
"ML models implemented from scratch using NumPy and Pandas only"

✅ Why XGBoost?

XGBoost is popular because it:

Delivers state-of-the-art accuracy

Prevents overfitting using regularization

Handles missing values automatically

Supports parallel and distributed computing

Works well with both small and large datasets



---

🔑 Key Features

Gradient Boosting Framework: Builds trees sequentially to minimize errors.

Regularization: L1 and L2 regularization to avoid overfitting.

Weighted Quantile Sketch: Efficient handling of sparse data.

Cross-Platform: Works with Python, R, Java, and more.



---

🛠 Use Cases

Classification: Customer churn prediction, fraud detection.

Regression: House price prediction, demand forecasting.

Ranking: Recommendation systems, search engines.

Time Series Forecasting



---

📌 Advantages over Other Algorithms

Faster training due to parallelization.

Better accuracy than traditional gradient boosting.

Built-in cross-validation and early stopping.

Handles missing values and categorical features efficiently.



---

🔍 When to Use XGBoost?

When accuracy is critical

When dataset is large and complex

When you need interpretable results with feature importance



---

📚 References

Official Documentation

Original Paper
---

📊 Visualizations

This project includes:

Feature_importances

ROC Curve:-0.88

Training loss over Iterations

Distribution of Predicted Probabilities



---

📈 Results

Accuracy: ~81%

ROC AUC: ~0.81
(Results may vary depending on preprocessing and hyperparameters)
Kaggle Guide to XGBoost
