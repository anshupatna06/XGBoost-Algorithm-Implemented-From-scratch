# XGBoost-Algorithm-Implemented-From-scratch
"ML models implemented from scratch using NumPy and Pandas only"

âœ… Why XGBoost?

XGBoost is popular because it:

Delivers state-of-the-art accuracy

Prevents overfitting using regularization

Handles missing values automatically

Supports parallel and distributed computing

Works well with both small and large datasets



---

ğŸ”‘ Key Features

Gradient Boosting Framework: Builds trees sequentially to minimize errors.

Regularization: L1 and L2 regularization to avoid overfitting.

Weighted Quantile Sketch: Efficient handling of sparse data.

Cross-Platform: Works with Python, R, Java, and more.



---

ğŸ›  Use Cases

Classification: Customer churn prediction, fraud detection.

Regression: House price prediction, demand forecasting.

Ranking: Recommendation systems, search engines.

Time Series Forecasting



---

ğŸ“Œ Advantages over Other Algorithms

Faster training due to parallelization.

Better accuracy than traditional gradient boosting.

Built-in cross-validation and early stopping.

Handles missing values and categorical features efficiently.



---

ğŸ” When to Use XGBoost?

When accuracy is critical

When dataset is large and complex

When you need interpretable results with feature importance



---

ğŸ“š References

Official Documentation

Original Paper
---

ğŸ“Š Visualizations

This project includes:

Feature_importances

ROC Curve:-0.88

Training loss over Iterations

Distribution of Predicted Probabilities



---

ğŸ“ˆ Results

Accuracy: ~81%

ROC AUC: ~0.81
(Results may vary depending on preprocessing and hyperparameters)
Kaggle Guide to XGBoost
